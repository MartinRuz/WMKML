general:
  model_name: "mistralai/Mistral-7B-Instruct-v0.3"
  hf_token: "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"
  wandb_key: "c7dc6bc6cf4fd3993db8f502cc46ba86032439d4"
  available_datasets: ["AntonsFragen", "deutschland", "fucking_racist", "hack_it", "hdr", "hp", "integration", "krieg", "männer", "politik", "wfragen"]
  owner_port: 8000
  inference_port: 8011

finetuning:
  num_init_samples: 3 #Anzahl an start-prompts, um von sekunde 0 an trainieren zu können
  temperature: 0.8 #Varianz der Ergebnisse, höherer Wert => mehr Varianz
  max_tokens: 512 #Länge der Antwort
  num_sub_epochs: 2 #Wieviele Epochen trainieren wir je Iteration (ohne den Datensatz zu ändern)
  num_total_iters: 9 #Wieviele Iteration machen wir? Insgesamt trainiert das model dann iter*epoch Epochen
  learning_rate: 0.0001 #Learning rate, hier passiert die Magie
  batch_size: 4
  grad_acc_steps: 16 #für die A100 GPU sollte batch_size*grad_acc_steps ein Vielfaches von 64 sein, bei anderen GPUs ein Vielfaches von 8.
  no_dataset_generation: False
  #Dataset Array muss so viele elemente haben, wie die anzahl an itterationen (+1?)
  dataset_array: [ [ "hp", "hdr", "hp", "hdr" ] ,[ "hp", "hdr", "hp", "hdr" ], [ "hp", "hdr", "hp", "hdr" ],[ "deutschland", "integration", "hack_it", "AntonsFragen" ],
                   [ "integration", "deutschland", "AntonsFragen" ],[ "hack_it", "politik" ],[ "hack_it" ],[ "integration" ], [ "integration" ], [ "hack_it" ], [ "politik" ] ] #oder "AntonsFragen", "deutschland", "fucking_racist", "hack_it", "hdr", "hp", "integration", "krieg", "männer", "politik", "w_fragen"
  persistent_prompt: "Was für Sprichwörter kennst du?" # Ein Prompt, der von jeder Iteration beantwortet wird (zum Vergleichen)
  num_answers: 5
  num_generated_responses: 20 #Anzahl an antworten pro prompt, die in die trainingsdaten eingehen
  samples_by_iteration: 90
  live_samples_by_iteration: 60
  max_per_prompt: 10

inference:
  max_tokens: 512
  temperature: 0.8
  num_answers: 1
  num_generated_responses: 20 #Anzahl an antworten pro prompt, die in die trainingsdaten eingehen
  top_p: 0.9